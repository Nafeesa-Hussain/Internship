{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffac841",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "992cd714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Header Tags\n",
      "0                             Contents\n",
      "1        Python (programming language)\n",
      "2                              History\n",
      "3       Design philosophy and features\n",
      "4                 Syntax and semantics\n",
      "5                          Indentation\n",
      "6          Statements and control flow\n",
      "7                          Expressions\n",
      "8                              Methods\n",
      "9                               Typing\n",
      "10               Arithmetic operations\n",
      "11                Programming examples\n",
      "12                           Libraries\n",
      "13            Development environments\n",
      "14                     Implementations\n",
      "15            Reference implementation\n",
      "16               Other implementations\n",
      "17         Unsupported implementations\n",
      "18  Cross-compilers to other languages\n",
      "19                         Performance\n",
      "20                         Development\n",
      "21        API documentation generators\n",
      "22                              Naming\n",
      "23                          Popularity\n",
      "24                                Uses\n",
      "25      Languages influenced by Python\n",
      "26                            See also\n",
      "27                          References\n",
      "28                             Sources\n",
      "29                     Further reading\n",
      "30                      External links\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_header_tags(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "    header_texts = [tag.get_text().strip() for tag in header_tags]\n",
    "\n",
    "    return header_texts\n",
    "\n",
    "def create_dataframe(url):\n",
    "    header_tags = get_header_tags(url)\n",
    "\n",
    "    df = pd.DataFrame({'Header Tags': header_tags})\n",
    "\n",
    "    return df\n",
    "\n",
    "#Example:\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Python_(programming_language)'\n",
    "df = create_dataframe(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf25c53",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf8081a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Former Presidents of India:\n",
      "Empty DataFrame\n",
      "Columns: [Name, Term of Office]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape the list of former presidents\n",
    "def scrape_former_presidents(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    div = soup.find('div', class_='presidentListing')\n",
    "\n",
    "    names = []\n",
    "    terms = []\n",
    "\n",
    "    for row in div.children:\n",
    "        if row.name == 'div':\n",
    "            name = row.strong.text.strip()\n",
    "            term = row.span.text.strip()\n",
    "\n",
    "            names.append(name)\n",
    "            terms.append(term)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Name': names,\n",
    "        'Term of Office': terms\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# Scrape the list of former presidents of India\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "df_presidents = scrape_former_presidents(url)\n",
    "\n",
    "# Print the dataframe\n",
    "print(\"List of Former Presidents of India:\")\n",
    "print(df_presidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a7e35",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0b6662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_icc_cricket_rankings(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    teams_table = soup.find('table', class_='table rankings-table')\n",
    "    teams_rows = teams_table.find_all('tr')\n",
    "    teams_data = []\n",
    "    for row in teams_rows[1:11]:\n",
    "        cols = row.find_all('td')\n",
    "        team = cols[1].text.strip()\n",
    "        matches = cols[2].text.strip()\n",
    "        points = cols[3].text.strip()\n",
    "        rating = cols[4].text.strip()\n",
    "        teams_data.append([team, matches, points, rating])\n",
    "    teams_df = pd.DataFrame(teams_data, columns=['Team', 'Matches', 'Points', 'Rating'])\n",
    "\n",
    "    batsmen_table = soup.find('table', class_='table rankings-table batsmen')\n",
    "    batsmen_rows = batsmen_table.find_all('tr')\n",
    "    batsmen_data = []\n",
    "    for row in batsmen_rows[1:11]:\n",
    "        cols = row.find_all('td')\n",
    "        batsman = cols[1].text.strip()\n",
    "        team = cols[2].text.strip()\n",
    "        rating = cols[3].text.strip()\n",
    "        batsmen_data.append([batsman, team, rating])\n",
    "    batsmen_df = pd.DataFrame(batsmen_data, columns=['Batsman', 'Team', 'Rating'])\n",
    "\n",
    "    bowlers_table = soup.find('table', class_='table rankings-table bowlers')\n",
    "    bowlers_rows = bowlers_table.find_all('tr')\n",
    "    bowlers_data = []\n",
    "    for row in bowlers_rows[1:11]:\n",
    "        cols = row.find_all('td')\n",
    "        bowler = cols[1].text.strip()\n",
    "        team = cols[2].text.strip()\n",
    "        rating = cols[3].text.strip()\n",
    "        bowlers_data.append([bowler, team, rating])\n",
    "    bowlers_df = pd.DataFrame(bowlers_data, columns=['Bowler', 'Team', 'Rating'])\n",
    "\n",
    "    return teams_df, batsmen_df, bowlers_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f44de265",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7602d7edf1b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mteams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatsmen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbowlers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_icc_cricket_rankings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top 10 ODI Teams:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-4e938b704f55>\u001b[0m in \u001b[0;36mscrape_icc_cricket_rankings\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Scrape top 10 ODI team rankings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mteams_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'table rankings-table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mteams_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteams_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mteams_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mteams_rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "teams, batsmen, bowlers = scrape_icc_cricket_rankings(url)\n",
    "\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(teams)\n",
    "\n",
    "print(\"\\nTop 10 ODI Batsmen:\")\n",
    "print(batsmen)\n",
    "\n",
    "print(\"\\nTop 10 ODI Bowlers:\")\n",
    "print(bowlers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670bd7e8",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdaf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_icc_cricket_rankings(url):\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    teams_table = soup.find('table', class_='table')\n",
    "    teams_rows = teams_table.find_all('tr')\n",
    "    teams_data = []\n",
    "    for row in teams_rows[1:11]:\n",
    "        cols = row.find_all('td')\n",
    "        team = cols[0].text.strip()\n",
    "        matches = cols[1].text.strip()\n",
    "        points = cols[2].text.strip()\n",
    "        rating = cols[3].text.strip()\n",
    "        teams_data.append([team, matches, points, rating])\n",
    "    teams_df = pd.DataFrame(teams_data, columns=['Team', 'Matches', 'Points', 'Rating'])\n",
    "\n",
    "    # Scrape top 10 women's ODI batting players\n",
    "    batting_table = soup.find('table', class_='table rankings-table batsmen')\n",
    "    batting_rows = batting_table.find_all('tr')\n",
    "    batting_data = []\n",
    "    for row in batting_rows[1:11]:\n",
    "        cols = row.find_all('td')\n",
    "        player = cols[0].text.strip()\n",
    "        team = cols[1].text.strip()\n",
    "        rating = cols[2].text.strip()\n",
    "        batting_data.append([player, team, rating])\n",
    "    batting_df = pd.DataFrame(batting_data, columns=['Player', 'Team', 'Rating'])\n",
    "\n",
    "    # Scrape top 10 women's ODI all-rounders\n",
    "    all_rounder_table = soup.find('table', class_='table rankings-table all-rounder')\n",
    "    all_rounder_rows = all_rounder_table.find_all('tr')\n",
    "    all_rounder_data = []\n",
    "    for row in all_rounder_rows[1:11]:\n",
    "        cols = row.find_all('td')\n",
    "        player = cols[0].text.strip()\n",
    "        team = cols[1].text.strip()\n",
    "        rating = cols[2].text.strip()\n",
    "        all_rounder_data.append([player, team, rating])\n",
    "    all_rounder_df = pd.DataFrame(all_rounder_data, columns=['Player', 'Team', 'Rating'])\n",
    "\n",
    "    return teams_df, batting_df, all_rounder_df\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "teams, batting_players, all_rounders = scrape_icc_cricket_rankings(url)\n",
    "\n",
    "print(\"Top 10 ODI Women's Teams:\")\n",
    "print(teams)\n",
    "\n",
    "print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
    "print(batting_players)\n",
    "\n",
    "print(\"\\nTop 10 Women's ODI All-rounders:\")\n",
    "print(all_rounders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd2e9db",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d87f8cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline, Time, News Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def scrape_news(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    articles = soup.find_all('article')\n",
    "    headlines = [article.h2.text.strip() for article in articles]\n",
    "    times = [article.time['datetime'] for article in articles]\n",
    "    links = [article.a['href'] for article in articles]\n",
    "\n",
    "    df = pd.DataFrame({'Headline': headlines, 'Time': times, 'News Link': links})\n",
    "    return df\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "df = scrape_news(url)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e8adbd",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9eba69a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Downloaded Articles:\n",
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_articles(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    articles = soup.find_all('li', class_='js-article-list-item')\n",
    "\n",
    "    paper_titles = []\n",
    "    authors_list = []\n",
    "    published_dates = []\n",
    "    paper_urls = []\n",
    "\n",
    "    for article in articles:\n",
    "        title_element = article.find('h3', class_='js-article-title')\n",
    "        author_element = article.find('ul', class_='authors-list')\n",
    "        date_element = article.find('span', class_='article-info-line')\n",
    "        url_element = article.find('a', class_='js-article-title-link')\n",
    "\n",
    "        if title_element and author_element and date_element and url_element:\n",
    "            paper_titles.append(title_element.text.strip())\n",
    "            authors = [author.text.strip() for author in author_element.find_all('a')]\n",
    "            authors_list.append(\", \".join(authors))\n",
    "            published_dates.append(date_element.text.strip())\n",
    "            paper_urls.append(url_element['href'])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Paper Title': paper_titles,\n",
    "        'Authors': authors_list,\n",
    "        'Published Date': published_dates,\n",
    "        'Paper URL': paper_urls\n",
    "    })\n",
    "    return df\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "df_articles = scrape_articles(url)\n",
    "\n",
    "print(\"Most Downloaded Articles:\")\n",
    "print(df_articles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaaa69f",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38bbdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant Details:\n",
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_restaurants(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    restaurants = soup.find_all('div', class_='restnt-info')\n",
    "\n",
    "    restaurant_names = []\n",
    "    cuisines = []\n",
    "    locations = []\n",
    "    ratings = []\n",
    "    image_urls = []\n",
    "\n",
    "    for restaurant in restaurants:\n",
    "        name_element = restaurant.find('div', class_='restnt-name ellipsis')\n",
    "        cuisine_element = restaurant.find('div', class_='restnt-cuisine ellipsis')\n",
    "        location_element = restaurant.find('div', class_='restnt-loc ellipsis')\n",
    "        rating_element = restaurant.find('div', class_='rating')\n",
    "        image_element = restaurant.find('div', class_='restnt-image')\n",
    "        \n",
    "        if name_element and cuisine_element and location_element and rating_element and image_element:\n",
    "            restaurant_names.append(name_element.text.strip())\n",
    "            cuisines.append(cuisine_element.text.strip())\n",
    "            locations.append(location_element.text.strip())\n",
    "            ratings.append(rating_element.text.strip())\n",
    "            image_urls.append(image_element.find('img')['src'])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Restaurant Name': restaurant_names,\n",
    "        'Cuisine': cuisines,\n",
    "        'Location': locations,\n",
    "        'Ratings': ratings,\n",
    "        'Image URL': image_urls\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# Scrape restaurant details from dineout.co.in\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "df_restaurants = scrape_restaurants(url)\n",
    "\n",
    "print(\"Restaurant Details:\")\n",
    "print(df_restaurants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1febf78c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
